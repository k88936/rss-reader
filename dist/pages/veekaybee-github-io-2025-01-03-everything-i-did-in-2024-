<p>I want to get back into writing more regularly this year, so in light of that, here&rsquo;s my last year in review.</p>
<h1 id="evaluating-llms">Evaluating LLMs</h1>
<p>Like many of us in tech, I spent a large portion of 2024 thinking about and working with LLMs, but I was lucky enough to do it for work. I spent the year designing, building, open-sourcing, (and naming! üêä) <a href="https://blog.mozilla.ai/lets-build-an-app-for-evaluating-llms/">an application to evaluate LLMs</a>, <a href="https://github.com/mozilla-ai/lumigator">Lumigator.</a></p>
<p>In support of that work, I <a href="https://blog.mozilla.ai/open-source-in-the-age-of-llms/">did open-source work in the LLM ecosystem</a>and learned a ton of stuff along the way. Just when I thought I had <a href="https://vickiboykis.com/2021/09/23/reaching-mle-machine-learning-enlightenment/">reached machine learning enlightenment</a>, I learned a ton about the weird <a href="https://vickiboykis.com/2024/05/06/weve-been-put-in-the-vibe-space/">nondeterministic properties of LLMs</a>, their evaluation methods, Ray + Ray Serve, vLLM, Llamafile, <a href="https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/">GGUF</a>, FastAPI, OpenAPI-compatible APIs, and much, much, much more. I am hoping to translate some of these into blog posts as well.</p>
<p>There is always more to learn in machine learning, particularly in the fast-moving world at the bleeding edge. <a href="https://vickiboykis.com/2022/11/10/how-i-learn-machine-learning/">Lucikly, the fundamentals always remain the same.</a></p>
<p>My biggest takeaways from this year are that:</p>
<ol>
<li><strong>Machine learning in industry is mostly still made up of good engineering</strong>, and in order to work with LLMs, you need to have both machine learning context and engineering discipline. There&rsquo;s some debate whether the &ldquo;old&rdquo; machine learning skills (gradient boosted trees anyone? üëµ) still apply in this Brave New World of <a href="https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/index.html">NonTabular Scraped Internet</a>, and <a href="https://timkellogg.me/blog/2024/12/10/ml-liability">I am of the opinion</a> that these skills are more valuable than ever, particularly as engineers without ML context enter the frustrating world of building with non-deterministic experimental loops.</li>
</ol>
<p>But what&rsquo;s even more valuable is being able to <a href="https://vickiboykis.com/2022/12/05/the-cloudy-layers-of-modern-day-programming/">ship things that hold engineering rigor</a>. Even if you are deep in academic research, <a href="https://bsky.app/profile/eugenevinitsky.bsky.social/post/3ldm5i4ljks2z">taking an engineering approach</a> will make your life significantly easier.</p>
<p>As part of this, I spent a significant amount of the year <a href="https://vickiboykis.com/2023/09/13/build-and-keep-your-context-window/">widening my own context window</a> about engineering best practices: Over the summer, I <a href="https://publish.obsidian.md/learning-c/Learning+C/Learn+C+Programming+and+OOP">learned C</a> and this fall, I learned JavaScript and Go and <a href="https://github.com/veekaybee/gitfeed">built my first working project, Gitfeed</a>. I am particularly enamored with Go: it&rsquo;s so boring, so clean, and so easy to build things that are simple and move quickly out of the box compared to the Python ecosystem.</p>
<p>This is not to say that the <a href="https://pydevtools.com/blog/effective-python-developer-tooling-in-december-2024/">Python world</a>, which is my first love, is not <a href="https://thedataquarry.com/posts/towards-a-unified-python-toolchain/">changing and evolving</a>. I <a href="https://bsky.app/profile/vickiboykis.com/post/3lazmcuftus25">moved to <code>uv</code> from <code>pyenv</code> </a> for both work and personal projects in 2024 and have never looked back, particularly as <code>uv</code> continues to work <a href="https://docs.astral.sh/uv/guides/integration/pytorch/">to gain support for PyTorch</a> and all of its cuda-related install issues.</p>
<ol start="2">
<li><strong>There is too much going on in the model landscape:</strong> too many models, too much choice, too many platforms, too much money, too much drama. My main strategy is to follow what&rsquo;s going on on a daily basis to keep my finger on the pulse and then basically ignore it. If people mention it again over the course of 2-3 months, continue to pay attention.</li>
</ol>
<p>In light of this, some contradictary advice if you&rsquo;re in the space is that you should also <strong>always be playing around with stuff</strong>, trying it out, building it and breakign it. There are some <a href="https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e">classic papers and books</a> for understanding the context and theory, but most of the stuff I read is still in <code> r/LocalLLaMA</code> or in various tweets/skeets and blog posts. We are not at the level yet for this stuff where cannon exists, although I have noticed that there are technical books being published about AI engineering this year, which means we are starting to solidify.</p>
<p>Some LLM tools I tried out and loved this year were <a href="https://github.com/Mozilla-Ocho/llamafile">llamafile</a>, for getting started with GGUF transformer and embedding models with a server extremely quickly, and <code>ollama + openwebui</code> for an experience that is nearly identical to using Claude. In fact, I&rsquo;ve switched over from Claude to using <code>mistral:latest</code> locally for most of my LLM usage, which is basically code search. Mistral is a model that has consistently worked well for me both at work and for my own purposes, and I&rsquo;ve enjoyed the chance to pit it against newer models like <a href="https://simonwillison.net/2024/Nov/12/qwen25-coder/">Qwen2.5 Coder</a>, Llama2, and phi.</p>
<p>I am still having a hard time wrapping my mind around what LLM models are useful for, in general, and for me personally. I <a href="https://mathstodon.xyz/@tao/113132502735585408">spend a lot</a> of time <a href="https://antirez.com/news/140">reading blog posts</a> about <a href="https://nicholas.carlini.com/writing/2024/how-i-use-ai.html">how other (smarter) people use LLMS</a> and against academic papers that study the success of these models at machin learning tasks stemming in their roots as NLP models focusing on problems like text completion, classification, translation, and summarization.</p>
<p>A very cool and fun related thing I did while reviewing the &ldquo;classics&rdquo; was edit this wonderful series of posts by Katharine about <a href="https://blog.kjamistan.com/a-deep-dive-into-memorization-in-deep-learning.html">model memorization</a> which you should read if you&rsquo;re interested in their internals.</p>
<ol start="3">
<li>I also spent a significant part of the year working on web development. I wrote earlier in the year that there is a <a href="https://blog.mozilla.ai/open-source-in-the-age-of-llms/">bifurcation in how we consume LLMs</a>:</li>
</ol>
<blockquote>
<p>The LLM ecosystem is currently bifurcated between HuggingFace and OpenAI compatibility: An interesting pattern has developed in my development work on open-source in LLMs. It‚Äôs become clear to me that, in this new space of developer tooling around transformer-style language models at an industrial scale, you are generally conforming to be downstream of one of two interfaces:
<strong>models that are trained and hosted using HuggingFace libraries</strong> and particularly the HuggingFace hub as infrastructure - in practicality, this means dependence on PyTorch‚Äôs programming paradigm, which HuggingFace tools wrap (although they now also provide interop between Tensorflow and JAX)</p>
</blockquote>
<blockquote>
<p><strong>Models that are available via API endpoints</strong>, particularly as hosted by OpenAI. Given that OpenAI was a first mover in the product LLM space, they currently have the API advantage, and many tools that have developed have developed OpenAI-compatible endpoints which don‚Äôt always mean using OpenAI, but conform to the same set of patterns that the Chat Completions API v1/chat/completions offers. For example, adding OpenAI-style interop chat completions allowed us to stand up our own vLLM OpenAI-compatible server that works against models we‚Äôve started with on HuggingFace and fine-tuned locally.</p>
</blockquote>
<blockquote>
<p>If you want to be successful in this space today and you&rsquo;d like to cater to a broad audience, you as a library or service provider have to be able to interface with both of these.</p>
</blockquote>
<p>I still believe this is true and it will be interesting to see how this develops over the course of the next year as APIs and their model ecosystems become even broader.</p>
<ol start="4">
<li><strong>These models will integrate with, not replace traditional machine learning systems.</strong> In the beginning, everyone thought that we could replace whole classes of engineering and machine learning problems with LLMs. It&rsquo;s becoming increasingly clearer that one big-ass model is not as a good as many smaller models for specific tasks performed in industry, which makes complete sense because LLMs as a concept rose out of academic labs whose goals are to generalize on out-of-sample problems, whereas the task of industry is to specialize in healthcare or OCR document scanning for legal, or sentiment analysis for social media. Even if fine-tuning did decline as a practice this year, people are making models specialized in a lot of deifferent ways, which I think the rise of agents at the end of this year really makes clear.</li>
</ol>
<p>An area that has particularly interested me is the integration of LLMs into recommender systems, my favorite area of applied machine learning. Earlier last year, I, along with <a href="https://jfkirk.github.io/posts/trustworthiness-ai/">James</a> and <a href="https://bsky.app/profile/ravimody.bsky.social">Ravi</a>, started a position paper on what will change in recommender systems in the light of LLMs based on a joke tweet I had made,</p>
<blockquote>
<p>personalized recommendations based on implicit matrix factorization from data acquired through large log streaming architectures were a low interest rate phenomenon</p>
</blockquote>
<p>My hypothesis was that companies used to collect a lot of streaming user data, particularly in social and other B2C settings, that were then used for personalization. Now, all the data collection happens publicly at the level of the interet, <a href="https://vickiboykis.com/2024/01/15/whats-new-with-ml-in-production/">where it is re-compressed into LLMs.</a> What does this mean for personalization, now?</p>
<p>Unfortunately we didn&rsquo;t finish the post (maybe someday?), but our main hypothesis was that traditional recsys will not be replaced with, but instead augmented with LLMs, that LLMs could help fill the cold-start gap because they&rsquo;re so good at zero-shot retrieval and general topic classification and can serve as places to augment cold-start problems and in onboarding, and that LLMs could offer more explainability in recommendations. This has <a href="https://arxiv.org/html/2410.17152v1">so far</a> <a href="https://arxiv.org/abs/2307.02046">proven to be true.</a></p>
<h1 id="italy--learning-italian">Italy + Learning Italian</h1>
<p>I had the extreme pleasure of being invited to give a keynote at <a href="https://2025.pycon.it/en">PyCon Italia</a> in Florence in May. I took all these learnings that had been running on crazed hamster wheels in my brain and threw them into a <a href="https://vickiboykis.com/2024/05/20/dont-worry-about-llms/">talk based loosely on the Decameron</a>.</p>
<p>In addition to subjecting my poor audience to jokes about the Medicis, I also gave the intro in Italian to an audience of +100, which was extremely nervewracking. I&rsquo;ve been wanting to study Italian since I was 18 and the stars aligned two years ago for me to seriously start. and I&rsquo;ve been learning ever singe. This year, I also managed to read my first (Level A1/A2) books in Italian.</p>
<figure><img src="https://veekaybee.github.io/images/902697ae-eefa-4315-bf30-f4603dc1489b.jpg" width="400">
</figure>

<p>My family also came to Italy. It was the kids&rsquo; first trip to Europe. Everyone had an amazing time (at least after my talk was over), ate a lot of gelato, and experienced Rome and Florence in the spring. One of my favorite moments of the trip (and of my year) was listening to a <a href="https://www.youtube.com/watch?v=6xTTHC2Y18E">Ricchi e Poveri</a> cover band playing at a disco outside our hotel at 11:30 at night in Florence.</p>
<figure><img src="https://veekaybee.github.io/images/bceed358-ffa0-4909-a353-834b30d215e4.jpg" width="400">
</figure>

<p>Inspired by all of this, and to take a break from LLMs, I also started learning how to mix music, and hopefully will have something more concrete to write/share about DJing this year.</p>
<p>I also <a href="https://vickiboykis.com/essays/2024-12-31-favorite-books/">managed to read some books.</a></p>
<h1 id="shift-in-social-strategy">Shift in Social Strategy</h1>
<p>As I spent the year trying my best to keep up with the wild, unruly, and growing LLM landscape, another part of my ecosystem was wilting. It&rsquo;s clear for all intents and purposes that Twitter as we knew her is dead. It doesn&rsquo;t have the juice anymore, as the kids say. I&rsquo;m surprised that it died from human intervention rather than engineering failure, but as anyone who&rsquo;s read my newsletter knows, this is by far the most likely outcome for techno-social systems. I have been wanting to write a long eulogy on my personal experinece and what Twitter meant to me: it was a place where I made friends, learned to be a serious programmer, worked out my best ideas, got leads for jobs, started a newsletter, <a href="https://vickiboykis.com/2022/12/22/everything-i-learned-about-accidentally-running-a-successful-tech-conference/">organized Normconf</a>, and most importantly, had fun. But in the end, it all died with a whimper, and I, like many others, just quietly left.</p>
<p>I&rsquo;ve always been a firm believer in owning your own internet space, and I continued to do that by blogging, and moving from Substack to Buttondown, but I also need micro social interaction because I get energy and ideas from it. So, I engaged more on Bluesky, where I&rsquo;ve been since last year, and was pleasantly surprised to see the machine learning community start to migrate there. Amost immediately as soon as I came back to the platform, I had a conversation that <a href="https://vickiboykis.com/2024/11/09/why-are-we-using-llms-as-calculators/">led to an idea for a post.</a></p>
<p>A lot of people have (rightfully so) sworn off agglomerated social <a href="https://vickiboykis.com/2024/09/19/dead-internet-souls/">in favor of group chats.</a> This makes complete sense to me, yet as a creator and someone who thrives on community, I was sorely missing this space until I came back to Bluesky. There is a lot of conversation around protocol versus platform and what it means for Bluesky the app versus Bluesky the protocol to succeed.</p>
<p>In particular, I encourage anyone interested in the space to read <a href="https://dustycloud.org/blog/how-decentralized-is-bluesky/">this series of exchanges</a> <a href="https://whtwnd.com/bnewbold.net/3lbvbtqrg5t2t">on the question</a> but I remain hopeful that that space grows and remains a place where users can choose and experiment. It allowed me, at the end of the year, <a href="https://github.com/veekaybee/gitfeed">to hack on gitfeed</a>, which is very, very very cool.</p>
<h1 id="2025">2025</h1>
<p>What next? Who knows, who can even predict the future? Not machine learning models, and certainly not me. I&rsquo;m just excited to keep learning and building and making and dreaming, and, hopefully writing more about it all.</p>